torchdistill: un marco modular basado en la configuraci贸n para la destilaci贸n del conocimiento
Versi贸n de PyPI Estado de construcci贸n

torchdistill (anteriormente kdkit ) ofrece varios m茅todos de destilaci贸n de conocimientos y le permite dise帽ar (nuevos) experimentos simplemente editando un archivo yaml en lugar de c贸digo Python. Incluso cuando necesite extraer representaciones intermedias en modelos profesor / alumno, NO necesitar谩 volver a implementar los modelos, que a menudo cambian la interfaz del reenv铆o, sino que especifiquen la (s) ruta (s) del m贸dulo en el archivo yaml.

Administrador de gancho hacia adelante
Con ForwardHookManager , puede extraer representaciones intermedias en el modelo sin modificar la interfaz de su funci贸n de avance.
Este cuaderno de ejemplo le dar谩 una mejor idea del uso.

Precisi贸n de validaci贸n de primer nivel para ILSVRC 2012 (ImageNet)
T: ResNet-34 *	Preentrenado	KD	A	PIE	CRD	Tf-KD	SSKD	L2	PAD-L2
S: ResNet-18	69,76 *	71,37	70,90	71,56	70,93	70,52	70.09	71.08	71,71
Trabajo original	N / A	N / A	70,70	71,43 **	71,17	70,42	71,62	70,90	71,71
* Torchvision proporciona los modelos ResNet-34 y ResNet-18 previamente entrenados.
** FT se eval煤a con ILSVRC 2015 en el trabajo original.
Para la segunda fila (S: ResNet-18), el punto de control (pesos entrenados), los archivos de configuraci贸n y de registro est谩n disponibles , y las configuraciones reutilizan los hiperpar谩metros, como el n煤mero de 茅pocas utilizadas en el trabajo original, excepto KD.

Ejemplos
El c贸digo ejecutable se puede encontrar en ejemplos / como

Clasificaci贸n de im谩genes : ImageNet (ILSVRC 2012), CIFAR-10, CIFAR-100, etc.
Detecci贸n de objetos : COCO 2017, etc.
Segmentaci贸n sem谩ntica : COCO 2017, PASCAL VOC, etc
Ejemplos de Google Colab
CIFAR-10 y CIFAR-100
Formaci贸n sin modelos docentes Abrir en Colab
Destilaci贸n del conocimiento Abrir en Colab
Estos ejemplos est谩n disponibles en demo / . Tenga en cuenta que los ejemplos son para usuarios de Google Colab y, por lo general, los ejemplos / ser铆an una mejor referencia si tiene sus propias GPU.

Citaci贸n
[ Preimpresi贸n ]

@article { matsubara2020torchdistill ,
   title = { torchdistill: A Modular, Configuration-Driven Framework for Knowledge Distillation } ,
   author = { Matsubara, Yoshitomo } ,
   year = { 2020 } 
  eprint = { 2011.12913 } ,
   archivePrefix = { arXiv } ,
   primaryClass = { cs.LG } 
}
Como instalar
Python 3.6> =
pipenv (opcional)
Instalar por pip / pipenv
pip3 install torchdistill
# or use pipenv
pipenv install torchdistill
Instalar desde este repositorio
git clone https://github.com/yoshitomo-matsubara/torchdistill.git
cd torchdistill/
pip3 install -e .
# or use pipenv
pipenv install "-e ."
Problemas / Contacto
La documentaci贸n est谩 en proceso. Mientras tanto, si茅ntase libre de crear un problema si tiene una solicitud de funci贸n o env铆eme un correo electr贸nico ( yoshitom@uci.edu ) si desea preguntarme en privado.

Referencias
 pytorch / visi贸n / referencias / clasificaci贸n /
 pytorch / visi贸n / referencias / detecci贸n /
 pytorch / vision / referencias / segmentaci贸n /
Geoffrey Hinton, Oriol Vinyals y Jeff Dean. "Destilar el conocimiento en una red neuronal" (Taller de aprendizaje profundo y aprendizaje de representaci贸n: NeurIPS 2014)
Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta y Yoshua Bengio. "FitNets: sugerencias para redes finas y profundas" (ICLR 2015)
Junho Yim, Donggyu Joo, Jihoon Bae y Junmo Kim. "Un regalo de la destilaci贸n del conocimiento: optimizaci贸n r谩pida, minimizaci贸n de la red y aprendizaje de transferencia" (CVPR 2017)
Sergey Zagoruyko y Nikos Komodakis. "Prestar m谩s atenci贸n a la atenci贸n: mejorar el rendimiento de las redes neuronales convolucionales mediante la transferencia de atenci贸n" (ICLR 2017)
Nikolaos Passalis y Anastasios Tefas. "Aprendizaje de representaciones profundas con transferencia probabil铆stica de conocimiento" (ECCV 2018)
Jangho Kim, Seonguk Park y Nojun Kwak. "Red compleja parafraseada: compresi贸n de red mediante transferencia de factores" (NeurIPS 2018)
Byeongho Heo, Minsik Lee, Sangdoo Yun y Jin Young Choi. "Transferencia de conocimiento a trav茅s de la destilaci贸n de los l铆mites de activaci贸n formados por neuronas ocultas" (AAAI 2019)
Wonpyo Park, Dongju Kim, Yan Lu y Minsu Cho. "Destilaci贸n del conocimiento relacional" (CVPR 2019)
Sungsoo Ahn, Shell Xu Hu, Andreas Damianou, Neil D. Lawrence y Zhenwen Dai. "Destilaci贸n de informaci贸n variacional para la transferencia de conocimientos" (CVPR 2019)
Yoshitomo Matsubara, Sabur Baidya, Davide Callegaro, Marco Levorato y Sameer Singh. "Redes neuronales profundas divididas destiladas para sistemas en tiempo real asistidos por bordes" (Taller sobre temas candentes en an谩lisis de video y bordes inteligentes: MobiCom 2019)
Baoyun Peng, Xiao Jin, Jiaheng Liu, Dongsheng Li, Yichao Wu, Yu Liu, Shunfeng Zhou y Zhaoning Zhang. "Congruencia de correlaci贸n para la destilaci贸n del conocimiento" (ICCV 2019)
Frederick Tung y Greg Mori. "Destilaci贸n del conocimiento que preserva la similitud" (ICCV 2019)
Yonglong Tian, Dilip Krishnan y Phillip Isola. "Destilaci贸n de representaci贸n contrastiva" (ICLR 2020)
Yoshitomo Matsubara y Marco Levorato. "Compresi贸n y filtrado neuronales para la detecci贸n de objetos en tiempo real asistida por bordes en redes desafiadas" (ICPR 2020)
Li Yuan, Francis EHTay, Guilin Li, Tao Wang y Jiashi Feng. "Revisando la destilaci贸n de conocimientos mediante la regularizaci贸n de suavizado de etiquetas" (CVPR 2020)
Guodong Xu, Ziwei Liu, Xiaoxiao Li y Chen cambian a Loy. "La destilaci贸n del conocimiento se encuentra con la auto-supervisi贸n" (ECCV 2020)
Youcai Zhang, Zhonghao Lan, Yuchen Dai, Fangao Zeng, Yan Bai, Jie Chang y Yichen Wei. "Destilaci贸n adaptativa Prime-Aware" (ECCV 2020)
