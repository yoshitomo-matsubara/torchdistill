Projects
=====


.. toctree::
   :maxdepth: 2
   :caption: Overview

This page is a showcase of OSS (open source software) and papers which have used **torchdistill** in the projects.
If your work is built on **torchdistill**, start `a "Show and tell" discussion at GitHub <https://github.com/yoshitomo-matsubara/torchdistill/discussions/new?category=show-and-tell>`_.


OSS
*****

sc2bench
----
* PyPI: https://pypi.org/project/sc2bench/
* Code: https://github.com/yoshitomo-matsubara/sc2-benchmark

This framework was built on PyTorch and designed to benchmark SC2 methods, *Supervised Compression for Split Computing*.
It is pip-installable and published as a PyPI package i.e., you can install it by :code:`pip3 install sc2bench`


-----

Papers
*****

FOOL: Addressing the Downlink Bottleneck in Satellite Computing With Neural Feature Compression
----
* Author(s): Alireza Furutanpey, Qiyang Zhang, Philipp Raith, Tobias Pfandzelter, Shangguang Wang, Schahram Dustdar
* Venue: IEEE Transactions on Mobile Computing
* PDF: `Paper <https://ieeexplore.ieee.org/document/10897922/>`_
* Code: `GitHub <https://github.com/rezafuru/the-fool>`_

**Abstract**: Nanosatellite constellations equipped with sensors capturing large geographic regions provide unprecedented opportunities for Earth observation.
As constellation sizes increase, network contention poses a downlink bottleneck. Orbital Edge Computing (OEC) leverages limited onboard compute resources to reduce transfer costs by processing the raw captures at the source. However, current solutions have limited practicality due to reliance on crude filtering methods or over-prioritizing particular downstream tasks. This work presents an OEC-native and task-agnostic feature compression method that preserves prediction performance and partitions high-resolution satellite imagery to maximize throughput. Further, it embeds context and leverages inter-tile dependencies to lower transfer costs with negligible overhead. While the encoding prioritizes features for downstream tasks, we can reliably recover images with competitive scores on quality measures at lower bitrates. We extensively evaluate transfer cost reduction by including the peculiarity of intermittently available network connections in low earth orbit. Lastly, we test the feasibility of our system for standardized nanosatellite form factors. We demonstrate that the proposed approach permits downlinking over 100$\times$ the data volume without relying on prior information on the downstream tasks.


A Multi-task Supervised Compression Model for Split Computing
----
* Author(s): Yoshitomo Matsubara, Matteo Mendula, Marco Levorato
* Venue: WACV 2025
* PDF: `Paper <https://arxiv.org/abs/2501.01420>`_
* Code: `GitHub <https://github.com/yoshitomo-matsubara/ladon-multi-task-sc2>`_

Split computing (â‰  split learning) is a promising approach to deep learning models for resource-constrained
edge computing systems, where weak sensor (mobile) devices are wirelessly connected to stronger edge servers through
channels with limited communication capacity. State-of-the-art work on split computing presents methods for single tasks
such as image classification, object detection, or semantic segmentation. The application of existing methods to
multitask problems degrades model accuracy and/or significantly increase runtime latency. In this study, we propose Ladon,
the first multi-task-head supervised compression model for multi-task split computing. Experimental results show that
the multi-task supervised compression model either outperformed or rivaled strong lightweight baseline models in terms
of predictive performance for ILSVRC 2012, COCO 2017, and PASCAL VOC 2012 datasets while learning compressed
representations at its early layers. Furthermore, our models reduced end-to-end latency (by up to 95.4%) and
energy consumption of mobile devices (by up to 88.2%) in multi-task split computing scenarios.


Understanding the Role of the Projector in Knowledge Distillation
----
* Author(s): Roy Miles, Krystian Mikolajczyk
* Venue: Proceedings of the 38th AAAI Conference on Artificial Intelligence (AAAI-24)
* PDF: `Paper <https://ojs.aaai.org/index.php/AAAI/article/view/28219/28433/>`_
* Code: `GitHub <https://github.com/roymiles/Simple-Recipe-Distillation>`_

**Abstract**: In this paper we revisit the efficacy of knowledge distillation as a function matching and metric learning
problem. In doing so we verify three important design decisions, namely the normalisation, soft maximum function, and
projection layers as key ingredients. We theoretically show that the projector implicitly encodes information on past
examples, enabling relational gradients for the student. We then show that the normalisation of representations is tightly
coupled with the training dynamics of this projector, which can have a large impact on the students performance.
Finally, we show that a simple soft maximum function can be used to address any significant capacity gap problems.
Experimental results on various benchmark datasets demonstrate that using these insights can lead to superior or
comparable performance to state-of-the-art knowledge distillation techniques, despite being much more computationally
efficient. In particular, we obtain these results across image classification (CIFAR100 and ImageNet), object detection
(COCO2017), and on more difficult distillation objectives, such as training data efficient transformers, whereby
we attain a 77.2% top-1 accuracy with DeiT-Ti on ImageNet. Code and models are publicly available.


FrankenSplit: Efficient Neural Feature Compression With Shallow Variational Bottleneck Injection for Mobile Edge Computing
----
* Author(s): Alireza Furutanpey, Philipp Raith, Schahram Dustdar
* Venue: IEEE Transactions on Mobile Computing
* PDF: `Paper <https://ieeexplore.ieee.org/document/10480247/>`_
* Code: `GitHub <https://github.com/rezafuru/FrankenSplit>`_

**Abstract**: The rise of mobile AI accelerators allows latency-sensitive applications to execute lightweight Deep
Neural Networks (DNNs) on the client side. However, critical applications require powerful models that edge devices
cannot host and must therefore offload requests, where the high-dimensional data will compete for limited bandwidth.
Split Computing (SC) alleviates resource inefficiency by partitioning DNN layers across devices, but current methods
are overly specific and only marginally reduce bandwidth consumption. This work proposes shifting away from focusing on
executing shallow layers of partitioned DNNs. Instead, it advocates concentrating the local resources on variational
compression optimized for machine interpretability. We introduce a novel framework for resource-conscious compression
models and extensively evaluate our method in an environment reflecting the asymmetric resource distribution between
edge devices and servers. Our method achieves 60% lower bitrate than a state-of-the-art SC method without decreasing
accuracy and is up to 16x faster than offloading with existing codec standards.


torchdistill Meets Hugging Face Libraries for Reproducible, Coding-Free Deep Learning Studies: A Case Study on NLP
----
* Author(s): Yoshitomo Matsubara
* Venue: EMNLP 2023 Workshop for Natural Language Processing Open Source Software (NLP-OSS)
* PDF: `Paper <https://aclanthology.org/2023.nlposs-1.18/>`_
* Code: `GitHub <https://github.com/yoshitomo-matsubara/torchdistill>`_

**Abstract**: Reproducibility in scientific work has been becoming increasingly important in research communities
such as machine learning, natural language processing, and computer vision communities due to the rapid development of
the research domains supported by recent advances in deep learning. In this work, we present a significantly upgraded
version of torchdistill, a modular-driven coding-free deep learning framework significantly upgraded from the initial
release, which supports only image classification and object detection tasks for reproducible knowledge distillation
experiments. To demonstrate that the upgraded framework can support more tasks with third-party libraries, we reproduce
the GLUE benchmark results of BERT models using a script based on the upgraded torchdistill, harmonizing with various
Hugging Face libraries. All the 27 fine-tuned BERT models and configurations to reproduce the results are published at
Hugging Face, and the model weights have already been widely used in research communities. We also reimplement popular
small-sized models and new knowledge distillation methods and perform additional experiments for computer vision tasks.


SC2 Benchmark: Supervised Compression for Split Computing
----
* Author(s): Yoshitomo Matsubara, Ruihan Yang, Marco Levorato, Stephan Mandt
* Venue: TMLR
* PDF: `Paper + Supp <https://openreview.net/forum?id=p28wv4G65d>`_
* Code: `GitHub <https://github.com/yoshitomo-matsubara/sc2-benchmark>`_

**Abstract**: With the increasing demand for deep learning models on mobile devices, splitting neural network
computation between the device and a more powerful edge server has become an attractive solution. However, existing
split computing approaches often underperform compared to a naive baseline of remote computation on compressed data.
Recent studies propose learning compressed representations that contain more relevant information for supervised
downstream tasks, showing improved tradeoffs between compressed data size and supervised performance. However, existing
evaluation metrics only provide an incomplete picture of split computing. This study introduces supervised compression
for split computing (SC2) and proposes new evaluation criteria: minimizing computation on the mobile device, minimizing
transmitted data size, and maximizing model accuracy. We conduct a comprehensive benchmark study using 10 baseline
methods, three computer vision tasks, and over 180 trained models, and discuss various aspects of SC2. We also release
our code and sc2bench, a Python package for future research on SC2. Our proposed metrics and package will help
researchers better understand the tradeoffs of supervised compression in split computing.


Supervised Compression for Resource-Constrained Edge Computing Systems
----
* Author(s): Yoshitomo Matsubara, Ruihan Yang, Marco Levorato, Stephan Mandt
* Venue: WACV 2022
* PDF: `Paper + Supp <https://openaccess.thecvf.com/content/WACV2022/html/Matsubara_Supervised_Compression_for_Resource-Constrained_Edge_Computing_Systems_WACV_2022_paper.html>`_
* Code: `GitHub <https://github.com/yoshitomo-matsubara/supervised-compression>`_

**Abstract**: There has been much interest in deploying deep learning algorithms on low-powered devices, including
smartphones, drones, and medical sensors. However, full-scale deep neural networks are often too resource-intensive
in terms of energy and storage. As a result, the bulk part of the machine learning operation is therefore often
carried out on an edge server, where the data is compressed and transmitted. However, compressing data (such as images)
leads to transmitting information irrelevant to the supervised task. Another popular approach is to split the deep
network between the device and the server while compressing intermediate features. To date, however, such split
computing strategies have barely outperformed the aforementioned naive data compression baselines due to their
inefficient approaches to feature compression. This paper adopts ideas from knowledge distillation and neural image
compression to compress intermediate feature representations more efficiently. Our supervised compression approach
uses a teacher model and a student model with a stochastic bottleneck and learnable prior for entropy coding
(Entropic Student). We compare our approach to various neural image and feature compression baselines in three vision
tasks and found that it achieves better supervised rate-distortion performance while maintaining smaller end-to-end
latency. We furthermore show that the learned feature representations can be tuned to serve multiple downstream tasks.

torchdistill: A Modular, Configuration-Driven Framework for Knowledge Distillation
----
* Author(s): Yoshitomo Matsubara
* Venue: ICPR 2020 International Workshop on Reproducible Research in Pattern Recognition
* PDF: `Paper <https://link.springer.com/chapter/10.1007/978-3-030-76423-4_3>`_
* Code: `GitHub <https://github.com/yoshitomo-matsubara/torchdistill>`_

**Abstract**: While knowledge distillation (transfer) has been attracting attentions from the research community,
the recent development in the fields has heightened the need for reproducible studies and highly generalized frameworks
to lower barriers to such high-quality, reproducible deep learning research. Several researchers voluntarily published
frameworks used in their knowledge distillation studies to help other interested researchers reproduce their original
work. Such frameworks, however, are usually neither well generalized nor maintained, thus researchers are still
required to write a lot of code to refactor/build on the frameworks for introducing new methods, models, datasets and
designing experiments. In this paper, we present our developed open-source framework built on PyTorch and dedicated for
knowledge distillation studies. The framework is designed to enable users to design experiments by declarative PyYAML
configuration files, and helps researchers complete the recently proposed ML Code Completeness Checklist. Using the
developed framework, we demonstrate its various efficient training strategies, and implement a variety of knowledge
distillation methods. We also reproduce some of their original experimental results on the ImageNet and COCO datasets
presented at major machine learning conferences such as ICLR, NeurIPS, CVPR and ECCV, including recent state-of-the-art
methods. All the source code, configurations, log files and trained model weights are publicly available at
https://github.com/yoshitomo-matsubara/torchdistill.
